dataset = "nyt"
sigma_norm = 1.0
weight_decay = 1e-2
tie_embedding = true
learning_rate = 1e-3
emb_dim = 300
num_encoder_layers = [1, 2] // (one of these is optimal)
encoder_dim = 300
drop = 0.2
emb_drop = [0.15, 0.25] // (one of these is optimal)
out_drop = [0.15, 0.25] // (one of these is optimal)
encoder_drop = 0.0
decoder_drop = 0.0
num_decoder_layers = 1
decoder_dim = 300
rank = 300
use_bpe = true
vocab_size = 11000 // 11000 if using bpe, else 124500
use_copy = true
reuse_attention = false
random_neighbor = false
use_cluster = false
encode_neighbor = true
sum_neighbor = false
att_neighbor = true
binary_neighbor = false
neighbor_dim = 150
num_neighbors = 10
max_enc_steps = 750
max_dec_steps = 400
beam_width = 10
max_grad_norm = 0.1
num_eval_steps = 10000
save_checkpoints_steps = 500
lr_schedule = 10000
total_steps = 150000
cp = 0.0
length_norm = 1.0
batch_size = 64
rnn_cell = "hyper_lstm"
att_type = "my"
use_bridge = true
use_residual = true
trainer = "adam"
sampling_probability = 0.25
num_mlp_layers = 1
sample_neighbor = false
