dataset = "giga"
use_bpe = true
vocab_size = 26000
use_copy = false
reuse_attention = false
random_neighbor = false
use_cluster = false
encode_neighbor = true
sum_neighbor = false
att_neighbor = false
binary_neighbor = false
binary_dim = 0
neighbor_dim = 32
num_neighbors = 10
max_enc_steps = 1000
max_dec_steps = 50
max_grad_norm = 1.0
num_eval_steps = 10000
save_checkpoints_steps = 5000
lr_schedule = 240000
total_steps = 1000000
beam_width = 10
length_norm = 1.0
coverage_penalty = 0.
batch_size = 64
rnn_cell = "hyper_lstm"
att_type = "luong"
use_bridge = true
use_residual = true
trainer = "adam"
num_mlp_layers = 1
sampling_probability = 0.0
sample_neighbor = false
weight_decay = 1e-2
tie_embedding = true
decoder_drop = 0.0
num_decoder_layers = 1
sigma_norm = 16.0
learning_rate = 1e-3
emb_dim = 256
num_encoder_layers = 3
encoder_dim = 256
drop = 0.15
emb_drop = [0.15, 0.25, 0.35] // (one of these is optimal)
out_drop = [0.15, 0.25, 0.35] // (one of these is optimal)
encoder_drop = [0.0, 0.1] // (one of these is optimal)
decoder_dim = 256
rank = 256
